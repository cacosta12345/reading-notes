# Reading Notes Class 16

## Reading Questions

1. What are the key differences between scraping static and dynamic websites?

    * Scraping static websites involves directly accessing and parsing the HTML code as it is served from the server, without needing to execute any Python. It's relatively straightforward because the content doesn't change after it's loaded.

    * Scraping dynamic websites, on the other hand, requires executing Python and often dealing with APIs or AJAX calls to render the content. This often means using tools that can mimic a browser's behavior to fully render the page before scraping, as the content may change based on user interactions or other asynchronous events.

2. Explain at least three techniques or best practices that can be employed to avoid getting blocked while scraping websites.

    * Respect Robots.txt  - Web spiders should ideally follow the robot.txt file for a website while scraping. It has specific rules for good behavior, such as how frequently you can scrape, which pages allow scraping, and which ones you canâ€™t.

    * Make the crawling slower - Web scraping bots fetch data very fast, but it is easy for a site to detect your scraper, as humans cannot browse that fast. The faster you crawl, the worse it is for everyone.

    * Do not follow the same crawling pattern - Incorporate some random clicks on the page, mouse movements and random actions that will make a spider look like a human.

3. What is Playwright, and how does it assist in web scraping tasks? Provide an example of a use case where Playwright would be particularly beneficial.

    * Playwright is an open-source automation library developed by Microsoft for testing and automating web browsers. It supports multiple browsers like Chrome, Firefox, and WebKit, enabling cross-browser testing. Playwright assists in web scraping tasks by allowing developers to programmatically navigate through web pages, interact with web elements, and extract data, even from dynamic websites that heavily rely on JavaScript.

    A use case where Playwright would be particularly beneficial is scraping data from a single-page application (SPA) that dynamically loads content based on user interactions. For example, extracting product details from an e-commerce site that uses infinite scrolling to load more items would be more efficiently done with Playwright, as it can automate the scrolling and wait for the dynamic content to load before scraping.

4. Describe the purpose of using Xpath in web scraping, and provide an example of an Xpath expression to select a specific HTML element from a webpage.

    * XPath is used in web scraping to navigate through the elements and attributes in an HTML document to select specific content based on its structure and attributes. It provides a way to query HTML documents to extract data by defining paths to the desired elements.

    Example of an XPath expression to select all paragraphs inside a div with a class name "content":

    //div[@class='content']/p
